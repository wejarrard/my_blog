---
title: A Dive into Resampling
author: Will Jarrard
date: '2022-06-30'
slug: []
categories: []
tags: [r, tidyverse]
type: ''
subtitle: ''
image: ''
---



<div id="required-r-packages-and-directories" class="section level1">
<h1>Required R packages and Directories</h1>
<pre class="r"><code>library(tidyverse) # functions for data manipulation
library(FNN)</code></pre>
</div>
<div id="the-poor-mans-bayesian-posterior-bootstrapping" class="section level1">
<h1>The Poor mans bayesian posterior (Bootstrapping)</h1>
<p>Ive always found it cool how bootstrapping can estimate a bayesian posterior solution without any priors. Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. Below we see the power of bootstrapping.</p>
<p>First, lets create a set of functions to generate data from the following distributions:</p>
<pre class="r"><code>#-- Simulation functions
sim_x &lt;- function(n) runif(n, 0, 2)     # U[0,2]
f &lt;- function(x) 1 + 2*x + 5*sin(5*x)   # true mean function
sim_y &lt;- function(x){                   # generate Y|X from N{f(x),sd}
  n = length(x)
  f(x) + rnorm(n, sd=2.5)
}</code></pre>
<p>lets start by simulating <span class="math inline">\(n=100\)</span> realizations from these distributions.</p>
<pre class="r"><code>#-- Generate Data
n = 100                                 # number of observations
set.seed(211)                           # set seed for reproducibility
x = sim_x(n)                            # get x values
y = sim_y(x)                            # get y values
data = tibble(x, y)

#-- Plot: ggplot
ggplot(data, aes(x,y)) +
  geom_point() +
  geom_function(fun=f, color=&quot;blue&quot;) +
  scale_x_continuous(breaks=seq(0, 2, by=.20))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Now, lets fit a 5th degree polynomial and draw the <em>estimated</em> regression curve.</p>
<pre class="r"><code>#-- Fit 5th degree polynomial
fit = lm(y~poly(x, degree=5), data=data)

#-- Make predictions
poly_data = tibble(x = seq(0, 2, length=100)) %&gt;% # evaluation points
  mutate(yhat = predict(fit, .))                  # predictions

#-- Plot: ggplot
ggplot(data, aes(x, y)) +
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, formula = &quot;y~poly(x, degree=5)&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Now I am going to draw 200 bootstrap samples, fit a 5th degree polynomial to each bootstrap sample, and make predictions</p>
<pre class="r"><code>eval.pts = seq(0, 2, length=100)

#-- Function that returns predictions at eval.pts
fit_poly &lt;- function(data, eval.pts) {
  fit = lm(y ~ poly(x, degree = 5), data=data)
  predict(fit, tibble(x=eval.pts))  # return predictions at eval.pts
}

#-- Run bootstrap: with loop
R = 200   # number of bootstrap samples
preds = matrix(NA, length(eval.pts), R)
set.seed(212)
for(j in 1:R){
  ind.boot = sample.int(n, replace=TRUE)
  preds[, j] = fit_poly(data[ind.boot,], eval.pts=eval.pts)
}

# add the eval.points and convert to long format to facilitate plotting
boot_data = preds %&gt;%
  as_tibble() %&gt;%          # convert to tibble
  mutate(x=eval.pts) %&gt;%   # add column with eval. points
  pivot_longer(-x, names_to=&quot;iter&quot;, values_to=&quot;y&quot;) # convert to long format</code></pre>
<pre><code>## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre class="r"><code>#-- Plots: ggplot2
boot_data %&gt;%
  ggplot(aes(x, y)) +
  geom_point(data=data) +
  geom_line(aes(group=iter), alpha=.1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>lets figure out the 95% confidence intervals from the bootstrap samples.</p>
<pre class="r"><code>CI = boot_data %&gt;%
  group_by(x) %&gt;%
  summarize(lower = quantile(y, probs=.025),
            upper = quantile(y, probs=.975))

#-- Plot: ggplot2
ggplot(data, aes(x, y)) +
  geom_point() +
  geom_line(data=poly_data, aes(y=yhat)) +
  # geom_line(data=boot_data, aes(group=iter), alpha=.1) +
  geom_line(data=CI, aes(y=lower), color=&quot;red&quot;, lty=2, size=1.05) +
  geom_line(data=CI, aes(y=upper), color=&quot;red&quot;, lty=2, size=1.05) +
  scale_x_continuous(breaks=seq(0, 2, by=.20))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="k-fold-cross-validation" class="section level1">
<h1>K-Fold cross-validation</h1>
<p>Im gonna run 10-fold cross-validation to select a K for k nearest neighbors.</p>
<pre class="r"><code>#-- Generate Data
n = 100                                 # number of observations
set.seed(211)                           # set seed for reproducibility
x = sim_x(n)                            # get x values
y = sim_y(x)                            # get y values
data = tibble(x, y)               # make into tibble


#- Get K-fold partition
set.seed(221)                      # set seed for replicability
n.folds = 10                       # number of folds for cross-validation
fold = sample(rep(1:n.folds, length=n))  # vector of fold labels
# notice how this is different than:  sample(1:K,n,replace=TRUE),
#  which won&#39;t give almost equal group sizes


#- Initialize
K = 3:50                                # set of k values for kNN
MSE = matrix(NA, length(K), n.folds)    # hold out of sample mse estiamtes
n.val = numeric(n.folds)                # number of elements in validation sets

#- Iterate over folds
for(j in 1:n.folds){

  #-- Set training/val data
  val = which(fold == j)
  train = which(fold != j)
  n.val[j] = length(val)

  #-- fit set of knn models
  for(i in 1:length(K)){
    k = K[i]
    knn = FNN::knn.reg(train = data[train,&#39;x&#39;, drop=FALSE],
                       y = data$y[train],
                       test = data[val,&#39;x&#39;, drop=FALSE],
                       k = k)
    r.val = data$y[val]-knn$pred      # residuals on val data
    MSE[i, j] = mean(r.val^2)
  }
}

#-- calculate CV error and standard error
CV = (MSE %*% n.val)/n               # Cross-validation error (MSE)
## CV = rowMeans(MSE)       # won&#39;t be exact if n.val has different counts
SE = apply(MSE, 1, sd)/sqrt(n.folds) # standard error

#-- Optimal Solution
results = tibble(k = K, MSE = CV[,1], SE)
results %&gt;%
  filter(min_rank(MSE) == 1)</code></pre>
<pre><code>## # A tibble: 1 × 3
##       k   MSE    SE
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     8  5.94 0.596</code></pre>
<pre class="r"><code>#-- Plot: ggplot
results %&gt;%
  mutate(optimal = ifelse(MSE == min(MSE), TRUE, FALSE)) %&gt;%
  ggplot(aes(K, color=optimal)) +
  geom_point(aes(y=MSE)) +
  geom_errorbar(aes(ymin=MSE - SE, ymax=MSE+SE)) +
  scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;)) +
  scale_x_continuous(breaks=seq(0, 50, by=5))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Now were gonna find the optimal <em>edf</em></p>
<pre class="r"><code>results = results %&gt;% mutate(edf = (n*9/10)/k)
results %&gt;% filter(MSE == min(MSE))</code></pre>
<pre><code>## # A tibble: 1 × 4
##       k   MSE    SE   edf
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     8  5.94 0.596  11.2</code></pre>
<pre class="r"><code>#-- Plot: ggplot
results %&gt;%
  mutate(optimal = ifelse(MSE == min(MSE), TRUE, FALSE)) %&gt;%
  ggplot(aes(edf, color=optimal)) +
  geom_point(aes(y=MSE)) +
  geom_errorbar(aes(ymin=MSE - SE, ymax=MSE+SE)) +
  scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>kNN models are attempting to fit a local mean by averaging the responses in a neighborhood of X. There is a bias-variance trade-off here; if the neighborhood grows too big there will be high bias, but if k is reduced then the variance increases.</p>
<p>As the size of the training data grows, the distance to the k neighbors will decrease and consequently the bias will be reduced but the variance will stay the same. This implies that the optimal k can potentially be increased (to reduce the variance) with a larger training data.</p>
<p>This is important in our cross-validation setup because we are training with smaller training data (90% smaller in the case of 10-fold cross-validation). Thus, building a model with the full training data can potentially use a larger k.</p>
<p>One way to adjust for this is to use the effective degrees of freedom (edf) instead of k to select the final model. Take the optimal edf from cross-validation and convert to the optimal k when using the full training data according to k∗=n/edf∗</p>
<pre class="r"><code>#-- optimal k on full training set
results %&gt;% filter(MSE == min(MSE)) %&gt;%
  mutate(k.full = n/edf,
         k.star = round(k.full))  # round since k must be an integer</code></pre>
<pre><code>## # A tibble: 1 × 6
##       k   MSE    SE   edf k.full k.star
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     8  5.94 0.596  11.2   8.89      9</code></pre>
<p>How well did our cross validation preform by simulating a ton more data from the same distribution!</p>
<pre class="r"><code>#-- Generate Test Data
ntest = 50000                           # Number of test samples
set.seed(223)                           # set *different* seed
xtest = sim_x(ntest)                    # generate test X&#39;s
ytest = sim_y(xtest)                    # generate test Y&#39;s

#-- Data
data.test = tibble(x=xtest, y=ytest)

#-- fit set of knn models
MSE.test = numeric(length(K))
for(i in 1:length(K)){
  k = K[i]
  knn = knn.reg(data[,&#39;x&#39;, drop=FALSE],
                y = data$y,
                test=data.test[,&#39;x&#39;, drop=FALSE],
                k=k)
  r.test = data.test$y-knn$pred      # residuals on test data
  MSE.test[i] = mean(r.test^2)
}

results.test = tibble(k=K, edf=n/k, MSE=MSE.test)
filter(results.test, MSE == min(MSE))</code></pre>
<pre><code>## # A tibble: 1 × 3
##       k   edf   MSE
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    13  7.69  7.11</code></pre>
<pre class="r"><code>#-- EDF on x-axis
bind_rows(cv = results,
          test = results.test, .id=&quot;error&quot;) %&gt;%
  ggplot(aes(edf, MSE, color=error)) +
  geom_point() + geom_line() +
  labs(title=&quot;MSE as function of EDF&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>#-- K on x-axis
bind_rows(cv = results,
          test = results.test, .id=&quot;error&quot;) %&gt;%
  ggplot(aes(k, MSE, color=error)) +
  geom_point() + geom_line() +
  labs(title=&quot;MSE as function of k&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<p>Not Bad! Although we selected the “wrong” tuning parameter, we still would have had very good performance - almost as good as using the optimal value of k according to the mse. We could do repeated holdout to make it even better</p>
</div>
